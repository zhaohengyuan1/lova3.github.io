<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LOVA3: Learning to Visual Question Answering, Asking and Assessment">
  <meta name="keywords" content="LOVA3, EvalQABench, MLLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LOVA3: Learning to Visual Question Answering, Asking and Assessment</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./assets/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="">
            Genixer
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">LOVA3: Learning to Visual Question Answering, Asking and Assessment</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=QLSk-6IAAAAJ">Henry Hengyuan Zhao</a><sup style="color: #ffac33;">1</sup>,</span>
            <span class="author-block">
              <a href="https://panzhous.github.io/">Pan Zhou</a><sup style="color: #ed4b82;">2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=No9OsocAAAAJ&hl=en">Difei Gao</a><sup style="color: #ffac33;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.baizechen.site">Zechen Bai</a><sup style="color: #ffac33;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/showlab">Mike Zheng Shou</a><sup style="color: #ffac33;">1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color: #ffac33;">1</sup>Show Lab, National University of Singapore,</span>
            <span class="author-block"><sup style="color: #ed4b82;">2</sup>Singapore Management University</span>
          </div>

          <div class="is-size-5">
            <p><b>in NeurIPS 2024</b></p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2405.14974"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.14974"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/showlab/LOVA3"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/hhenryz/LOVA3-llava-v1.5-7b"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-share-square"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/hhenryz/Mixed_VQA_GenQA_EvalQA_1.5M"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Training Dataset</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/hhenryz/EvalQABench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>EvalQABench</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Question answering, asking, and assessment are three innate human traits crucial for 
            understanding the world and acquiring knowledge. By enhancing these capabilities, 
            humans can more effectively utilize data, leading to better comprehension and learning outcomes. 
            Current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often 
            neglecting the full potential of questioning and assessment skills. 
            Inspired by the human learning mechanism, we introduce <b>LOVA3</b>, an innovative framework named 
            "Learning tO Visual question Answering, Asking and Assessment," designed to equip MLLMs with 
            these additional capabilities. 
          </p>
          <p>
            Our approach involves the creation of two supplementary training tasks <b>GenQA</b> and <b>EvalQA</b>, 
            aiming at fostering the skills of asking and assessing questions in the context of images. 
            To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. 
            For assessment, we introduce a new benchmark called <b>EvalQABench</b>, comprising 64,000 training samples 
            (split evenly between positive and negative samples) and 5,000 validation and testing samples. 
            We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions will enhance their multimodal 
            comprehension, ultimately improving overall performance. To validate this hypothesis, we train MLLMs using the LOVA3 
            framework and evaluate them on a range of multimodal datasets and benchmarks. Our results demonstrate consistent performance gains, 
            underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Contributions</h2>
        <p style="text-align: left;"><b style="color: #FA8072;">LOVA3</b> - To the best of our knowledge, LOVA3 is the first effort to imbue the asking and assessment abilities in training 
          a robust and intelligent MLLM, inspired from human learning mechanism.</p>
        <p style="text-align: left;"><b style="color: #FA8072;">EvalQABench</b> -  We build a new benchmark EvalQABench for the VQA correction evaluation as the first effort to advance the 
          development of future research.</p>
        <p style="text-align: left;">
          <b style="color: #FA8072;">Performance Improvement</b> - Training with our proposed LOVA3 framework, we observe consistent improvement on 10 representative benchmarks.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">GenQA: Learn to generate diverse VQA pairs for unlabeled images</h2>
    <img src="assets/GenQAData.png" alt="" style="width: 100%">
    <p style="text-align: left;">If one MLLM is able to successfully generate high-quality question-answer pairs based on visual input, 
      it indicates a stronger problem-solving ability and deep visual understanding. 
      To enable the MLLM to ask questions, it is natural for us to gather existing annotated datasets as the training 
      corpus and then train the model to predict both questions and answers.</p>
  </div>
</div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">EvalQA: Learn to assess the correctness of VQA triplet</h2>
      <h3 class="title is-4">Automatic Data Generation Pipeline</h3>
      <img src="assets/EvalqaPipeline.png" alt="" style="width: 100%">
      <p style="text-align: left;">(1) We demonstrate that leveraging non-commercial MLLMs/LLMs is sufficient to building negative answer and feedback.
      <br>(2) We conduct manual data filtering and correction thanks to the sationary error patterns by using offline MLLM and LLM.</p>
      <img src="assets/manualfilter.png" alt="">
      <p>Stationary error patterns.</p>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-4">EvalQABench</h2>
      <p style="text-align: left;">we introduce a new benchmark, EvalQABench, to address the problem of assessing visual question-answering data.
        Moreover, instead of merely labeling each VQA pair with "Yes/No", we advocate for integrating feedback into each instance, an important 
        aspect rarely seen in prior multimodal benchmarks.
        <br>
        In total, EvalQABench comprise 64,000
        training samples (split evenly between positive and negative samples) and 5,000
        validation and testing samples.
      </p>
      <br>
      <br>
      <h2 class="title is-4">Selected 9 representative data types</h2>
      <img src="assets/evalqa_visual.png" alt="">
      <p style="text-align: left;"> (1) The question type of our EvalQABench is diverse. <br> (2) All candidate samples for data creation are from VQAv2 training set. </p>
      <br>
      <br>
      <h2 class="title is-4">Results of EvalQABench</h2>
      <img src="assets/evalqabenchresult.png" alt="" style="width: 100%">
      <br>
      <br>
      <h2 class="title is-4">Data Statistic</h2>
      <img src="assets/evalqa_datastatistic.png" alt="" style="width: 100%">
    </div>
  </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results by training on LLaVA1.5</h2>
        <img src="assets/result1.png" alt="" style="width: 100%">
        <img src="assets/result2.png" alt="" style="width: 100%">
        <img src="assets/result3.png" alt="" style="width: 100%">
        <p style="text-align: left;">
          (1) We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions 
          will enhance their multimodal comprehension, ultimately improving overall performance.
          <br>
          (2) We do not tune any hyperparameters of training LLaVA1.5.
          <br>
          (3) There is no extra data annotations used for GenQA task. For EvalQA task, we just provide negative answer as the additional annotations.
        </p>
      </div>
    </div>
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhao2024lova3learningvisualquestion,
      title={LOVA3: Learning to Visual Question Answering, Asking and Assessment}, 
      author={Henry Hengyuan Zhao and Pan Zhou and Difei Gao and Zechen Bai and Mike Zheng Shou},
      year={2024},
      eprint={2405.14974},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.14974}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
         <p>
          This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, 
          licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
         </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
